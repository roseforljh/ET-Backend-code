import orjson
import re
import logging
import datetime
from typing import Any, Dict, List, Tuple, Optional
import os
import uuid

from fastapi.responses import JSONResponse
from fastapi import UploadFile, Depends
from fastapi.security import OAuth2PasswordBearer
from ..models.api_models import User
 
from ..core.config import (
    COMMON_HEADERS,
    MAX_SSE_LINE_LENGTH,
    SUPPORTED_DOCUMENT_MIME_TYPES_FOR_TEXT_EXTRACTION,
    MAX_DOCUMENT_CONTENT_CHARS_FOR_PROMPT,
)

try:
    from google.cloud import storage
    from google.auth.exceptions import DefaultCredentialsError
except ImportError:
    storage = None # type: ignore
    DefaultCredentialsError = None # type: ignore
    logging.warning(
        "google-cloud-storage or google-auth library not found. "
        "GCS upload functionality for large files (video/audio) for Gemini will not be available."
    )

try:
    import PyPDF2
except ImportError:
    PyPDF2 = None
    logging.warning("1PyPDF2 library not found. PDF text extraction will not be available.")

try:
    import docx
except ImportError:
    docx = None
    logging.warning("python-docx library not found. DOCX text extraction will not be available.")

try:
    import olefile
except ImportError:
    olefile = None
    logging.warning("olefile library not found. DOC text extraction will not be available.")

try:
    import openpyxl
    import xlrd
except ImportError:
    openpyxl = None
    xlrd = None
    logging.warning("openpyxl/xlrd libraries not found. Excel text extraction will not be available.")

try:
    from pptx import Presentation
    pptx_available = True
except ImportError:
    pptx_available = False
    logging.warning("python-pptx library not found. PowerPoint text extraction will not be available.")

try:
    from bs4 import BeautifulSoup
    bs4_available = True
except ImportError:
    bs4_available = False
    logging.warning("BeautifulSoup4 library not found. HTML text extraction will be limited.")


logger = logging.getLogger("EzTalkProxy.Utils")

# å¿«é€Ÿæ£€æµ‹æ˜¯å¦ç–‘ä¼¼åŒ…å« Markdown ç»“æ„ï¼Œç”¨äºè¯­éŸ³æ¨¡å¼å…œåº•æ¸…æ´—çš„æ—©æœŸé€€å‡º
_MD_QUICK_PATTERN = re.compile(
    r"(^\s{0,3}#{1,6}\s)"          # æ ‡é¢˜è¡Œ: # / ## / ...
    r"|(^\s*[-*+]\s+)"            # æ— åºåˆ—è¡¨: - item
    r"|(^\s*\d+\.\s+)"            # æœ‰åºåˆ—è¡¨: 1. item
    r"|(```)"                     # ä»£ç å—å›´æ 
    r"|(`[^`]+`)"                 # è¡Œå†…ä»£ç 
    r"|(\[.+?\]\(.+?\))"          # é“¾æ¥ [text](url)
    r"|(!\[.*?\]\(.*?\))"         # å›¾ç‰‡ ![alt](url)
    r"|(^\s*\|.+\|)"              # è¡¨æ ¼è¡Œ
    r"|(\*\*.+?\*\*)"             # ç²—ä½“ **text**
    ,
    re.MULTILINE | re.DOTALL,
)


def strip_markdown_for_tts(text: str) -> str:
    """
    è¯­éŸ³æ¨¡å¼å…œåº•ï¼šåœ¨é€å…¥ TTS å‰å¯¹æ˜æ˜¾çš„ Markdown ç»“æ„åšè½»é‡æ¸…æ´—ã€‚
    è®¾è®¡åŸåˆ™ï¼š
    - æ£€æµ‹ä¸æ¸…æ´—éƒ½å¿…é¡»æ˜¯ O(n) ä¸”éå¸¸å¿«
    - è‹¥æ–‡æœ¬æœ¬èº«æ²¡æœ‰æ˜æ˜¾ Markdown ç»“æ„ï¼Œåˆ™ç›´æ¥åŸæ ·è¿”å›ï¼Œä¸åšä»»ä½•ä¿®æ”¹
    - å°½é‡åˆ é™¤/æ”¹å†™æ’ç‰ˆç¬¦å·ï¼Œä¿ç•™è¯­ä¹‰å†…å®¹
    """
    if not isinstance(text, str) or not text:
        return text

    # å¿«é€Ÿæ£€æµ‹ï¼šç»å¤§å¤šæ•°æ­£å¸¸å£è¯­åœ¨è¿™é‡Œç›´æ¥è¿”å›ï¼Œé¿å…å¤šè½®æ­£åˆ™
    if not _MD_QUICK_PATTERN.search(text):
        return text

    cleaned = text

    # 1. å»æ‰å›´æ ä»£ç å— ``` ```ï¼ˆæ•´ä½“ç§»é™¤ï¼Œé¿å…è¯»ä»£ç å™ªéŸ³ï¼‰
    cleaned = re.sub(
        r"```[a-zA-Z0-9_+\-]*\n.*?\n```",
        "",
        cleaned,
        flags=re.DOTALL,
    )

    # 2. è¡Œé¦–æ ‡é¢˜ã€å¼•ç”¨ã€åˆ—è¡¨æ ‡è®°
    cleaned = re.sub(r"^\s{0,3}#{1,6}\s*", "", cleaned, flags=re.MULTILINE)
    cleaned = re.sub(r"^\s{0,3}>\s?", "", cleaned, flags=re.MULTILINE)
    cleaned = re.sub(r"^\s*[-*+]\s+", "", cleaned, flags=re.MULTILINE)
    cleaned = re.sub(r"^\s*\d+\.\s+", "", cleaned, flags=re.MULTILINE)

    # 3. æ°´å¹³åˆ†éš”çº¿ï¼ˆ---ã€*** ç­‰ï¼‰
    cleaned = re.sub(
        r"^\s{0,3}[-*_]{3,}\s*$",
        "",
        cleaned,
        flags=re.MULTILINE,
    )

    # 4. é“¾æ¥ä¸å›¾ç‰‡ï¼š[text](url)ã€![alt](url) -> ä»…ä¿ç•™æ–‡å­—éƒ¨åˆ†
    cleaned = re.sub(
        r"!\[([^\]]*)\]\([^)]+\)",
        r"\1",
        cleaned,
    )
    cleaned = re.sub(
        r"\[([^\]]+)\]\([^)]+\)",
        r"\1",
        cleaned,
    )

    # 5. è¡Œå†…ä»£ç ä¸åŠ ç²—/æ–œä½“ï¼šå»æ‰ç¬¦å·ï¼Œä¿ç•™å†…å®¹
    cleaned = re.sub(r"`([^`]+)`", r"\1", cleaned)
    cleaned = re.sub(r"\*\*([^*]+)\*\*", r"\1", cleaned)
    cleaned = re.sub(r"\*([^*]+)\*", r"\1", cleaned)

    # 6. è¡¨æ ¼ä¸­ç«–çº¿ï¼šå°† | æ›¿æ¢ä¸ºé¡¿å·/é€—å·ï¼Œé¿å…è¢«è¯»å‡ºä¸ºå¥‡æ€ªç¬¦å·
    # è¿™é‡Œä¸è¿‡åº¦åŒºåˆ†æ˜¯ä¸æ˜¯è¡¨æ ¼ï¼Œåªè¦å‡ºç°å¤§é‡ | å°±åšæ¸©å’Œæ›¿æ¢
    cleaned = re.sub(r"\|+", "ï¼Œ", cleaned)

    # 7. æ”¶å°¾ï¼šæ¸…ç†å¤šä½™ç©ºç™½ä¸ç©ºè¡Œ
    cleaned = re.sub(r"[ \t]+\n", "\n", cleaned)
    cleaned = re.sub(r"\n{3,}", "\n\n", cleaned)
    cleaned = cleaned.strip()

    return cleaned


def orjson_dumps_bytes_wrapper(data: Any) -> bytes:
    return orjson.dumps(
        data,
        option=orjson.OPT_NON_STR_KEYS | orjson.OPT_PASSTHROUGH_DATETIME | orjson.OPT_APPEND_NEWLINE
    )

def to_sse_bytes(event) -> bytes:
    """
    å°† AppStreamEventPy åºåˆ—åŒ–ä¸ºç¬¦åˆ SSE è§„èŒƒçš„å­—èŠ‚ï¼š
    - å‰ç¼€ä½¿ç”¨ "data: "ï¼ˆæ³¨æ„åé¢çš„ç©ºæ ¼ï¼Œå…¼å®¹æ€§æ›´å¥½ï¼‰
    - æ¯æ¡äº‹ä»¶ä»¥åŒæ¢è¡Œç»“å°¾ \\n\\n
    - å…è®¸è¾“å…¥å·²å¸¦ "data:" çš„è½½è·ï¼ˆå¹‚ç­‰å¤„ç†ï¼‰
    """
    try:
        # ç»Ÿä¸€ç”Ÿæˆ JSON bytesï¼ˆæœ«å°¾é€šå¸¸å¸¦ä¸€ä¸ª \\nï¼‰
        payload = orjson_dumps_bytes_wrapper(
            event.model_dump(by_alias=True, exclude_none=True)
        )
        # å¦‚æœä¸Šæ¸¸å·²å¸¦ "data:" å‰ç¼€ï¼Œä¿ç•™ï¼›å¦åˆ™è¡¥å…… "data: "
        if payload.startswith(b"data:"):
            line = payload
        else:
            line = b"data: " + payload

        # ç¡®ä¿ä»¥ä¸¤ä¸ªæ¢è¡Œç»“æŸï¼ˆSSEäº‹ä»¶åˆ†éš”ï¼‰
        if line.endswith(b"\n\n"):
            return line
        elif line.endswith(b"\n"):
            return line + b"\n"
        else:
            return line + b"\n\n"
    except Exception:  # å…œåº•ï¼šä»»ä½•å¼‚å¸¸éƒ½è¿”å›ä¸€ä¸ªå¯ç”¨çš„ error äº‹ä»¶ï¼Œé¿å…å®¢æˆ·ç«¯æŒ‚èµ·
        fallback = {
            "type": "error",
            "message": "SSE serialization failure",
        }
        fb = orjson_dumps_bytes_wrapper(fallback)
        if not fb.startswith(b"data:"):
            fb = b"data: " + fb
        if fb.endswith(b"\n\n"):
            return fb
        elif fb.endswith(b"\n"):
            return fb + b"\n"
        else:
            return fb + b"\n\n"
def error_response(
    code: int,
    msg: str,
    request_id: Optional[str] = None,
    headers: Optional[Dict[str, str]] = None
) -> JSONResponse:
    log_msg = f"é”™è¯¯ {code}: {msg}"
    if request_id:
        log_msg = f"RID-{request_id}: {log_msg}"
    logger.warning(log_msg)
    
    final_headers = {**COMMON_HEADERS, **(headers or {})}
    
    return JSONResponse(
        status_code=code,
        content={"error": {"message": msg, "code": code, "type": "proxy_error"}},
        headers=final_headers
    )


def extract_sse_lines(buffer: bytearray) -> Tuple[List[bytes], bytearray]:
    lines: List[bytes] = []
    start_index: int = 0
    buffer_len = len(buffer)
    while start_index < buffer_len:
        newline_index = buffer.find(b'\n', start_index)
        if newline_index == -1:
            break
        line = buffer[start_index:newline_index]
        if line.endswith(b'\r'):
            line = line[:-1]
        if len(line) > MAX_SSE_LINE_LENGTH:
            logger.warning(
                f"SSE line too long ({len(line)} bytes), exceeded MAX_SSE_LINE_LENGTH ({MAX_SSE_LINE_LENGTH}). Line skipped. "
                f"Content start: {line[:100]!r}"
            )
        else:
            lines.append(line)
        start_index = newline_index + 1
    return lines, buffer[start_index:]

def get_current_time_iso() -> str:
    return datetime.datetime.utcnow().isoformat() + "Z"

def is_gemini_2_5_model(model_name: str) -> bool:
    if not isinstance(model_name, str):
        return False
    return "gemini-2.5" in model_name.lower() or "gemini-2.5-flash-image-preview" in model_name.lower()

def _extract_text_from_pdf_pypdf2(file_path: str) -> Optional[str]:
    if not PyPDF2:
        logger.warning("Attempted to extract PDF text, but PyPDF2 library is not available.")
        return None
    text_content = ""
    try:
        with open(file_path, "rb") as f:
            reader = PyPDF2.PdfReader(f)
            if reader.is_encrypted:
                try:
                    if reader.decrypt("") == PyPDF2.PasswordType.OWNER_PASSWORD or \
                       reader.decrypt("") == PyPDF2.PasswordType.USER_PASSWORD :
                        logger.info(f"Successfully decrypted PDF (with empty password): {file_path}")
                    else:
                        logger.warning(f"PDF file is encrypted and could not be decrypted with an empty password: {file_path}")
                        return None
                except Exception as decrypt_err:
                    logger.warning(f"Failed to decrypt PDF {file_path}: {decrypt_err}")
                    return None

            for page in reader.pages:
                try:
                    text_content += page.extract_text() or ""
                except Exception as page_extract_err:
                    logger.warning(f"Error extracting text from a page in {file_path}: {page_extract_err}")
                    continue
        return text_content.strip()
    except FileNotFoundError:
        logger.error(f"PDF file not found for extraction: {file_path}")
        return None
    except Exception as e:
        logger.error(f"Error extracting text from PDF {file_path} using PyPDF2: {e}", exc_info=True)
        return None

def _extract_text_from_docx_python_docx(file_path: str) -> Optional[str]:
    if not docx:
        logger.warning("Attempted to extract DOCX text, but python-docx library is not available.")
        return None
    try:
        doc_obj = docx.Document(file_path)
        full_text = [para.text for para in doc_obj.paragraphs]
        return "\n".join(full_text).strip()
    except FileNotFoundError:
        logger.error(f"DOCX file not found for extraction: {file_path}")
        return None
    except Exception as e:
        logger.error(f"Error extracting text from DOCX {file_path} using python-docx: {e}", exc_info=True)
        return None

def _extract_text_from_doc_olefile(file_path: str) -> Optional[str]:
    """ä½¿ç”¨olefileåº“ä».docæ–‡æ¡£ä¸­æå–æ–‡æœ¬ï¼ˆå¢å¼ºç‰ˆï¼Œæ”¯æŒä¸­æ–‡ï¼Œæ™ºèƒ½ç¼–ç æ£€æµ‹ï¼‰"""
    if not olefile:
        logger.warning("Attempted to extract DOC text, but olefile library is not available.")
        return None
    
    content = None
    try:
        # å°è¯•é€šè¿‡ OLE ç»“æ„è¯»å– WordDocument æµï¼Œå‡å°‘äºŒè¿›åˆ¶å™ªå£°
        if olefile.isOleFile(file_path):
            with olefile.OleFileIO(file_path) as ole:
                if ole.exists('WordDocument'):
                    with ole.openstream('WordDocument') as stream:
                        content = stream.read()
    except Exception as e:
        logger.warning(f"Failed to read WordDocument stream from {file_path}: {e}")
    
    # å¦‚æœè¯»å–æµå¤±è´¥ï¼Œå›é€€åˆ°è¯»å–æ•´ä¸ªæ–‡ä»¶
    if content is None:
        try:
            with open(file_path, 'rb') as f:
                content = f.read()
        except Exception as e:
            logger.error(f"Failed to read file {file_path}: {e}")
            return None

    # è¾…åŠ©å‡½æ•°ï¼šè¯„ä¼°æ–‡æœ¬çœ‹èµ·æ¥æ˜¯å¦åƒæ­£å¸¸çš„ä¸­æ–‡/è‹±æ–‡æ–‡æœ¬
    def score_text_validity(text: str) -> float:
        if not text: return 0.0
        length = len(text)
        if length == 0: return 0.0
        
        # å¸¸ç”¨ä¸­æ–‡é«˜é¢‘å­—å’Œæ ‡ç‚¹
        common_chars = {'çš„', 'ä¸€', 'æ˜¯', 'åœ¨', 'ä¸', 'äº†', 'æœ‰', 'å’Œ', 'äºº', 'è¿™', 'ä¸­', 'å¤§', 'ä¸º', 'ä¸Š', 'ä¸ª', 'å›½', 'æˆ‘', 'ä»¥', 'è¦', 'ä»–', 'æ—¶', 'æ¥', 'ç”¨', 'ä»¬', 'ç”Ÿ', 'åˆ°', 'ä½œ', 'åœ°', 'äº', 'å‡º', 'å°±', 'åˆ†', 'å¯¹', 'æˆ', 'ä¼š', 'å¯', 'ä¸»', 'å‘', 'å¹´', 'åŠ¨', 'åŒ', 'å·¥', 'ä¹Ÿ', 'èƒ½', 'ä¸‹', 'è¿‡', 'å­', 'è¯´', 'äº§', 'ç§', 'é¢', 'è€Œ', 'æ–¹', 'å', 'å¤š', 'å®š', 'è¡Œ', 'å­¦', 'æ³•', 'æ‰€', 'æ°‘', 'å¾—', 'ç»', 'å', 'ä¸‰', 'ä¹‹', 'è¿›', 'ç€', 'ç­‰', 'éƒ¨', 'åº¦', 'å®¶', 'ç”µ', 'åŠ›', 'é‡Œ', 'å¦‚', 'æ°´', 'åŒ–', 'é«˜', 'è‡ª', 'äºŒ', 'ç†', 'èµ·', 'å°', 'ç‰©', 'ç°', 'å®', 'åŠ ', 'é‡', 'éƒ½', 'ä¸¤', 'ä½“', 'åˆ¶', 'æœº', 'å½“', 'ä½¿', 'ç‚¹', 'ä»ä¸š', 'æœ¬', 'å»', 'å¿ƒ', 'ç•Œ', 'ä¹‰', 'ç¤¾', 'åˆ', 'å¹³', 'å£«', 'å‘Š', 'å¤–', 'æ²¡', 'çœ‹', 'æ', 'é‚£', 'é—®', 'æŒ‡', 'æ°”', 'åš', 'é‚»', 'è¥¿', 'çœŸ', 'å±±', 'å†…', 'æœˆ', 'å…¬', 'å…¨', 'ä¿¡', 'æœŸ', 'å®‰', 'æˆ–', 'ä¹¦', 'é—¨', 'åº”', 'è·¯', 'åˆ©', 'æ‰‹', 'æœ€', 'æ–°', 'ä¸–', 'ä½', 'åœº', 'å˜', 'å¾—', 'å‘˜', 'è¡¨', 'å£', 'å¸¸', 'å…³', 'äº‰', 'å†›', 'ç›®', 'è€…', 'æ¬¡', 'è§£', 'æ–‡', 'ä¹', 'å…«', 'æ— ', 'ç›¸', 'æ—¥', 'å¤–', 'åˆš', 'ä½†', 'æ­¥', 'å', 'å»º', 'æœ', 'æ–™', 'å¼ ', 'æ¥', 'å‘˜', 'å¸', 'ä½', 'å®', 'è¿', 'é€š', 'å†œ', 'ä¿', 'å¯¼', 'é›†', 'ç‰©', 'å±•', 'è±¡', 'å®Œ', 'é™¢', 'æ ·', 'å¹²', 'å¹¶', 'åˆ©', 'çœ', 'æº', 'å®‰', 'åƒ', 'ä¼—', 'æ•ˆ', 'ç®¡', 'æ¥', 'è§‰', 'èº«', 'ç¾', 'æ„', 'å…ˆ', 'é‡‘', 'æœˆ', 'å›', 'å·¥', 'çƒ­', 'æ€§', 'éŸ³', 'è€', 'åˆ‡', 'çº§', 'ç”±', 'å› ', 'è”', 'å³', 'ç™¾', 'çŸ¥', 'è¡¨', 'é˜Ÿ', 'ç»„', 'å†³', 'æ²»', 'çœ‹', 'ä½', 'ç¾', 'ç‚¹', 'é¢˜', 'ï¼Œ', 'ã€‚', 'ã€', 'ï¼›', 'ï¼š', 'ï¼Ÿ', 'ï¼', 'â€œ', 'â€', 'ï¼ˆ', 'ï¼‰'}
        
        score = 0
        for char in text:
            if char in common_chars:
                score += 1
            elif 'a' <= char <= 'z' or 'A' <= char <= 'Z' or '0' <= char <= '9':
                score += 0.1 # è‹±æ–‡æ•°å­—æƒé‡ä½ä¸€ç‚¹ï¼Œé¿å…å…¨è‹±æ–‡äºŒè¿›åˆ¶å™ªå£°å¹²æ‰°
        
        return score / length

    candidates = []
    import re

    # æ–¹æ³•1ï¼šUTF-16LE (Wordé»˜è®¤)
    try:
        # å¡«å……åˆ°å¶æ•°é•¿åº¦
        content_le = content + b'\0' if len(content) % 2 != 0 else content
        # è§£ç å¹¶è¿‡æ»¤æ— æ•ˆå­—ç¬¦
        text_le = content_le.decode('utf-16-le', errors='ignore')
        # æ¸…æ´—ï¼šåªä¿ç•™ CJKã€ASCII å’Œå¸¸è§ç¬¦å·
        # åŒ¹é…è¿ç»­çš„æœ‰æ•ˆå­—ç¬¦å—ï¼ˆè‡³å°‘2ä¸ªå­—ç¬¦ï¼‰ï¼Œå‡å°‘å•å­—å™ªå£°
        matches = re.findall(r'[\u4e00-\u9fff\x20-\x7e\uff00-\uffef\u3000-\u303f\t\n\r]{2,}', text_le)
        cleaned_le = "".join(matches)
        score_le = score_text_validity(cleaned_le)
        candidates.append((score_le, cleaned_le, "utf-16-le"))
    except Exception:
        pass

    # æ–¹æ³•2ï¼šGB18030 (ä¸­æ–‡æ–‡æ¡£å¸¸è§ï¼Œå°¤å…¶æ˜¯è€æ–‡æ¡£)
    try:
        text_gb = content.decode('gb18030', errors='ignore')
        matches = re.findall(r'[\u4e00-\u9fff\x20-\x7e\uff00-\uffef\u3000-\u303f\t\n\r]{2,}', text_gb)
        cleaned_gb = "".join(matches)
        score_gb = score_text_validity(cleaned_gb)
        candidates.append((score_gb, cleaned_gb, "gb18030"))
    except Exception:
        pass

    # é€‰å–å¾—åˆ†æœ€é«˜çš„
    candidates.sort(key=lambda x: x[0], reverse=True)
    
    if not candidates:
        return None

    best_score, best_text, best_encoding = candidates[0]
    
    # é˜ˆå€¼ï¼šè‡³å°‘æœ‰1%çš„å­—ç¬¦æ˜¯å¸¸è§å­—ç¬¦ï¼Œå¦åˆ™è®¤ä¸ºæ˜¯ä¹±ç 
    if best_score > 0.01 and len(best_text) > 5:
        logger.info(f"Extracted .doc text using {best_encoding}, score={best_score:.4f}, length={len(best_text)}")
        return best_text
    
    logger.warning(f"Extraction failed: best score {best_score:.4f} too low. Encoding: {best_encoding}")
    return None

def _extract_text_from_excel(file_path: str, mime_type: str) -> Optional[str]:
    """ä»Excelæ–‡ä»¶ä¸­æå–æ–‡æœ¬"""
    try:
        text_content = []
        
        if mime_type == "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet":
            # .xlsx æ–‡ä»¶
            if not openpyxl:
                logger.warning("openpyxl not available for .xlsx extraction")
                return None
                
            workbook = openpyxl.load_workbook(file_path)
            for sheet_name in workbook.sheetnames:
                sheet = workbook[sheet_name]
                text_content.append(f"=== Sheet: {sheet_name} ===")
                
                for row in sheet.iter_rows(values_only=True):
                    row_text = []
                    for cell in row:
                        if cell is not None:
                            row_text.append(str(cell))
                    if row_text:
                        text_content.append(" | ".join(row_text))
                        
        elif mime_type == "application/vnd.ms-excel":
            # .xls æ–‡ä»¶
            if not xlrd:
                logger.warning("xlrd not available for .xls extraction")
                return None
                
            workbook = xlrd.open_workbook(file_path)
            for sheet_idx in range(workbook.nsheets):
                sheet = workbook.sheet_by_index(sheet_idx)
                text_content.append(f"=== Sheet: {sheet.name} ===")
                
                for row_idx in range(sheet.nrows):
                    row_text = []
                    for col_idx in range(sheet.ncols):
                        cell = sheet.cell(row_idx, col_idx)
                        if cell.value:
                            row_text.append(str(cell.value))
                    if row_text:
                        text_content.append(" | ".join(row_text))
        
        return "\n".join(text_content).strip() if text_content else None
        
    except Exception as e:
        logger.error(f"Error extracting text from Excel {file_path}: {e}", exc_info=True)
        return None

def _extract_text_from_powerpoint(file_path: str) -> Optional[str]:
    """ä»PowerPointæ–‡ä»¶ä¸­æå–æ–‡æœ¬"""
    if not pptx_available:
        logger.warning("python-pptx not available for PowerPoint extraction")
        return None
        
    try:
        presentation = Presentation(file_path)
        text_content = []
        
        for slide_num, slide in enumerate(presentation.slides, 1):
            text_content.append(f"=== Slide {slide_num} ===")
            
            for shape in slide.shapes:
                if hasattr(shape, "text") and shape.text.strip():
                    text_content.append(shape.text.strip())
                    
        return "\n".join(text_content).strip() if text_content else None
        
    except Exception as e:
        logger.error(f"Error extracting text from PowerPoint {file_path}: {e}", exc_info=True)
        return None

def _extract_text_from_html(file_path: str) -> Optional[str]:
    """ä»HTMLæ–‡ä»¶ä¸­æå–æ–‡æœ¬"""
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            html_content = f.read()
            
        if bs4_available:
            # ä½¿ç”¨BeautifulSoupè§£æHTML
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ç§»é™¤scriptå’Œstyleæ ‡ç­¾
            for script in soup(["script", "style"]):
                script.decompose()
                
            # æå–æ–‡æœ¬
            text = soup.get_text()
            
            # æ¸…ç†å¤šä½™çš„ç©ºç™½
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = ' '.join(chunk for chunk in chunks if chunk)
            
            return text.strip() if text else None
        else:
            # ç®€å•çš„HTMLæ ‡ç­¾ç§»é™¤
            import re
            # ç§»é™¤HTMLæ ‡ç­¾
            text = re.sub(r'<[^>]+>', '', html_content)
            # æ¸…ç†å¤šä½™ç©ºç™½
            text = re.sub(r'\s+', ' ', text)
            return text.strip() if text else None
            
    except Exception as e:
        logger.error(f"Error extracting text from HTML {file_path}: {e}", exc_info=True)
        return None

def _extract_text_from_xml(file_path: str) -> Optional[str]:
    """ä»XMLæ–‡ä»¶ä¸­æå–æ–‡æœ¬"""
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            xml_content = f.read()
            
        if bs4_available:
            soup = BeautifulSoup(xml_content, 'xml')
            text = soup.get_text()
            # æ¸…ç†å¤šä½™çš„ç©ºç™½
            lines = (line.strip() for line in text.splitlines())
            text = '\n'.join(line for line in lines if line)
            return text.strip() if text else None
        else:
            # ç®€å•çš„XMLæ ‡ç­¾ç§»é™¤
            import re
            text = re.sub(r'<[^>]+>', '', xml_content)
            text = re.sub(r'\s+', ' ', text)
            return text.strip() if text else None
            
    except Exception as e:
        logger.error(f"Error extracting text from XML {file_path}: {e}", exc_info=True)
        return None

def _extract_text_from_json(file_path: str) -> Optional[str]:
    """ä»JSONæ–‡ä»¶ä¸­æå–æ–‡æœ¬å†…å®¹"""
    try:
        import json
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            data = json.load(f)
            
        def extract_strings(obj, path=""):
            """é€’å½’æå–JSONä¸­çš„å­—ç¬¦ä¸²å€¼"""
            strings = []
            if isinstance(obj, dict):
                for key, value in obj.items():
                    new_path = f"{path}.{key}" if path else key
                    strings.extend(extract_strings(value, new_path))
            elif isinstance(obj, list):
                for i, item in enumerate(obj):
                    new_path = f"{path}[{i}]"
                    strings.extend(extract_strings(item, new_path))
            elif isinstance(obj, str) and obj.strip():
                strings.append(f"{path}: {obj}")
            elif obj is not None:
                strings.append(f"{path}: {str(obj)}")
            return strings
            
        text_parts = extract_strings(data)
        return "\n".join(text_parts) if text_parts else None
        
    except Exception as e:
        logger.error(f"Error extracting text from JSON {file_path}: {e}", exc_info=True)
        return None

def _extract_text_from_plain_text(file_path: str) -> Optional[str]:
    common_encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1', 'iso-8859-1']
    try:
        for encoding in common_encodings:
            try:
                with open(file_path, "r", encoding=encoding) as f:
                    return f.read().strip()
            except UnicodeDecodeError:
                logger.debug(f"Failed to decode plain text file {file_path} with encoding {encoding}")
                continue
            except FileNotFoundError:
                logger.error(f"Plain text file not found for extraction: {file_path}")
                return None
        logger.warning(f"Could not decode plain text file {file_path} with common encodings.")
        return None
    except Exception as e:
        logger.error(f"Error extracting text from plain text file {file_path}: {e}", exc_info=True)
        return None


async def extract_text_from_uploaded_document(
    uploaded_file_path: str,
    mime_type: Optional[str],
    original_filename: str
) -> Optional[str]:
    logger.info(f"Attempting to extract text from '{original_filename}' (path: {uploaded_file_path}, mime: {mime_type})")
    effective_mime_type = mime_type.lower() if mime_type else None

    if not effective_mime_type:
        logger.warning(f"No effective MIME type for '{original_filename}', cannot determine extraction method.")
        return None

    extracted_text: Optional[str] = None

    if effective_mime_type in SUPPORTED_DOCUMENT_MIME_TYPES_FOR_TEXT_EXTRACTION:
        # Microsoft Office Documents
        if effective_mime_type == "application/pdf":
            extracted_text = _extract_text_from_pdf_pypdf2(uploaded_file_path)
        elif effective_mime_type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
            extracted_text = _extract_text_from_docx_python_docx(uploaded_file_path)
        elif effective_mime_type == "application/msword":
            logger.warning(f"ğŸ”¥ .docæ ¼å¼æ–‡æ¡£å¤„ç†ï¼š'{original_filename}' - .docæ ¼å¼è¾ƒè€ï¼Œæå–æ•ˆæœå¯èƒ½ä¸ä½³")
            extracted_text = _extract_text_from_doc_olefile(uploaded_file_path)
            if not extracted_text or len(extracted_text.strip()) < 10:
                extracted_text = f"""[æ–‡æ¡£è§£ææç¤º]

.docæ ¼å¼æ–‡æ¡£ '{original_filename}' çš„å†…å®¹æå–é‡åˆ°å›°éš¾ã€‚

å¯èƒ½åŸå› ï¼š
1. .docæ˜¯è¾ƒè€çš„Microsoft Wordæ ¼å¼ï¼Œç»“æ„å¤æ‚
2. æ–‡æ¡£å¯èƒ½åŒ…å«ç‰¹æ®Šæ ¼å¼æˆ–åŠ å¯†ä¿æŠ¤
3. å½“å‰è§£æå™¨å¯¹å¤æ‚.docæ–‡æ¡£æ”¯æŒæœ‰é™

å»ºè®®è§£å†³æ–¹æ¡ˆï¼š
1. å°†æ–‡æ¡£è½¬æ¢ä¸º.docxæ ¼å¼åé‡æ–°ä¸Šä¼ 
2. å°†æ–‡æ¡£å¦å­˜ä¸ºPDFæ ¼å¼åé‡æ–°ä¸Šä¼   
3. å¤åˆ¶æ–‡æ¡£å†…å®¹åˆ°çº¯æ–‡æœ¬æ–‡ä»¶(.txt)åä¸Šä¼ 

å¦‚éœ€å¸®åŠ©è½¬æ¢æ–‡æ¡£æ ¼å¼ï¼Œè¯·å‘ŠçŸ¥å…·ä½“éœ€æ±‚ã€‚"""
                logger.warning(f"Failed to extract meaningful content from .doc file '{original_filename}'")
        
        # Excel Documents
        elif effective_mime_type in ["application/vnd.openxmlformats-officedocument.spreadsheetml.sheet", "application/vnd.ms-excel"]:
            extracted_text = _extract_text_from_excel(uploaded_file_path, effective_mime_type)
        
        # PowerPoint Documents  
        elif effective_mime_type in ["application/vnd.openxmlformats-officedocument.presentationml.presentation", "application/vnd.ms-powerpoint"]:
            extracted_text = _extract_text_from_powerpoint(uploaded_file_path)
        
        # Web & Markup Documents
        elif effective_mime_type == "text/html":
            extracted_text = _extract_text_from_html(uploaded_file_path)
        elif effective_mime_type in ["text/xml", "application/xml"]:
            extracted_text = _extract_text_from_xml(uploaded_file_path)
        elif effective_mime_type == "application/json":
            extracted_text = _extract_text_from_json(uploaded_file_path)
        
        # Plain Text & Other Formats
        elif effective_mime_type.startswith("text/"):
            extracted_text = _extract_text_from_plain_text(uploaded_file_path)
        else:
            logger.info(f"MIME type '{effective_mime_type}' for '{original_filename}' is in supported list but no specific extractor implemented, attempting plain text.")
            extracted_text = _extract_text_from_plain_text(uploaded_file_path)
    else:
        logger.warning(f"Unsupported MIME type for text extraction: '{effective_mime_type}' for file '{original_filename}'.")
        return None

    if extracted_text:
        if len(extracted_text) > MAX_DOCUMENT_CONTENT_CHARS_FOR_PROMPT:
            logger.info(f"Extracted text from '{original_filename}' truncated from {len(extracted_text)} to {MAX_DOCUMENT_CONTENT_CHARS_FOR_PROMPT} characters.")
            extracted_text = extracted_text[:MAX_DOCUMENT_CONTENT_CHARS_FOR_PROMPT] + \
                             f"\n[å†…å®¹å·²æˆªæ–­ï¼ŒåŸå§‹é•¿åº¦è¶…è¿‡ {MAX_DOCUMENT_CONTENT_CHARS_FOR_PROMPT} å­—ç¬¦]"
        logger.info(f"Successfully extracted text (len: {len(extracted_text)}) from '{original_filename}'.")
        return extracted_text.strip()
    else:
        logger.warning(f"Failed to extract text from '{original_filename}' (mime: {effective_mime_type}).")
        return None

async def upload_to_gcs(
    file_obj: Any,
    original_filename: str,
    bucket_name: str,
    project_id: Optional[str] = None,
    content_type: Optional[str] = None,
    request_id: Optional[str] = None
) -> Optional[str]:
    log_prefix = f"RID-{request_id}" if request_id else "[GCS_UPLOAD]"
    
    if not storage:
        logger.error(f"{log_prefix} GCS upload skipped: google-cloud-storage library not available.")
        return None
    if not bucket_name:
        logger.error(f"{log_prefix} GCS upload skipped: GCS_BUCKET_NAME is not configured.")
        return None

    _, file_extension = os.path.splitext(original_filename)
    safe_original_filename_part = "".join(c if c.isalnum() or c in ['.', '_', '-'] else '_' for c in original_filename.rsplit('.', 1)[0])[:50]
    destination_blob_name = f"uploads/{request_id or 'unknown_req'}/{safe_original_filename_part}_{uuid.uuid4().hex[:8]}{file_extension}"

    logger.info(f"{log_prefix} Attempting to upload to GCS: bucket='{bucket_name}', blob='{destination_blob_name}'")

    try:
        if project_id:
            storage_client = storage.Client(project=project_id)
        else:
            storage_client = storage.Client()
            
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(destination_blob_name)

        if isinstance(file_obj, UploadFile):
            await file_obj.seek(0)
            blob.upload_from_file(file_obj.file, content_type=content_type or file_obj.content_type)
        elif hasattr(file_obj, 'read') and hasattr(file_obj, 'seek'):
            file_obj.seek(0)
            blob.upload_from_file(file_obj, content_type=content_type)
        elif isinstance(file_obj, str) and os.path.exists(file_obj):
            blob.upload_from_filename(file_obj, content_type=content_type)
        else:
            logger.error(f"{log_prefix} GCS upload failed for '{original_filename}': Invalid file_obj type or file path does not exist.")
            return None

        gcs_uri = f"gs://{bucket_name}/{destination_blob_name}"
        logger.info(f"{log_prefix} Successfully uploaded '{original_filename}' to GCS: {gcs_uri}")
        return gcs_uri
    except DefaultCredentialsError:
        logger.error(
            f"{log_prefix} GCS upload failed for '{original_filename}': Google Cloud Default Credentials not found. "
            "Ensure GOOGLE_APPLICATION_CREDENTIALS environment variable is set correctly "
            "or the runtime environment has appropriate GCS permissions.",
            exc_info=True
        )
        return None
    except Exception as e:
        logger.error(f"{log_prefix} GCS upload failed for '{original_filename}' (blob '{destination_blob_name}'): {e}", exc_info=True)
        return None

oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token")

async def get_current_user(token: str = Depends(oauth2_scheme)):
   # In a real application, you would verify the token and fetch the user from a database.
   # For now, we'll just return a dummy user.
   return User(username="johndoe", email="johndoe@example.com", full_name="John Doe")