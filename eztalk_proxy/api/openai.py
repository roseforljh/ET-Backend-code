import os
import logging
import httpx
import orjson
import asyncio
import base64
import uuid
from typing import Dict, Any, AsyncGenerator, List
from io import BytesIO

try:
    from PIL import Image
except ImportError:
    Image = None
    logging.warning("Pillow library not found. Image resizing optimization will not be available.")

from fastapi import Request, UploadFile
from fastapi.responses import StreamingResponse

from ..models.api_models import (
    ChatRequestModel,
    SimpleTextApiMessagePy,
    PartsApiMessagePy,
    AppStreamEventPy,
    PyTextContentPart,
    PyInlineDataContentPart,
    PyInputAudioContentPart
)
from ..core.config import (
    TEMP_UPLOAD_DIR,
    API_TIMEOUT,
    GEMINI_SUPPORTED_UPLOAD_MIMETYPES,
    DEFAULT_TEXT_API_URL,
    DEFAULT_TEXT_API_KEY
)
from ..utils.helpers import (
    orjson_dumps_bytes_wrapper,
    extract_text_from_uploaded_document
)
from ..services.requests.facade import prepare_openai_request
from ..services.streaming import (
    process_openai_like_sse_stream,
    handle_stream_error,
    handle_stream_cleanup,
)
from ..services.web_search import perform_web_search, generate_search_context_message_content
from ..services.zhipu_web_search import ZhipuWebSearchService

logger = logging.getLogger("EzTalkProxy.Handlers.OpenAI")

def _sse_event_bytes(event: AppStreamEventPy) -> bytes:
    """Serialize an AppStreamEvent to proper SSE line(s)."""
    payload = orjson_dumps_bytes_wrapper(event.model_dump(by_alias=True, exclude_none=True))
    if payload.startswith(b"data:"):
        return payload if payload.endswith(b"\n\n") else payload + b"\n\n"
    return b"data:" + payload + b"\n\n"

SUPPORTED_IMAGE_MIME_TYPES_FOR_OPENAI = ["image/jpeg", "image/png", "image/gif", "image/webp"]

# éŸ³é¢‘å’Œè§†é¢‘ç±»å‹å®šä¹‰
AUDIO_MIME_TYPES = [
    "audio/wav", "audio/x-wav", "audio/mpeg", "audio/mp3", "audio/aac", "audio/ogg",
    "audio/opus", "audio/flac", "audio/3gpp", "audio/amr", "audio/aiff", "audio/x-m4a",
    "audio/midi", "audio/webm"
]
VIDEO_MIME_TYPES = [
    "video/mp4", "video/mpeg", "video/quicktime", "video/x-msvideo", "video/x-flv",
    "video/x-matroska", "video/webm", "video/x-ms-wmv", "video/3gpp", "video/x-m4v"
]

def get_audio_format_from_mime_type(mime_type: str) -> str:
    """ä»MIMEç±»å‹æå–éŸ³é¢‘æ ¼å¼ï¼Œç”¨äºOpenAIå…¼å®¹æ ¼å¼"""
    mime_to_format = {
        "audio/wav": "wav",
        "audio/x-wav": "wav",
        "audio/mpeg": "mp3",
        "audio/mp3": "mp3",
        "audio/aac": "aac",
        "audio/ogg": "ogg",
        "audio/opus": "opus",
        "audio/flac": "flac",
        "audio/3gpp": "3gp",
        "audio/amr": "amr",
        "audio/aiff": "aiff",
        "audio/x-m4a": "m4a",
        "audio/midi": "midi",
        "audio/webm": "webm"
    }
    return mime_to_format.get(mime_type.lower(), mime_type.split('/')[-1])

def is_gemini_model(model_name: str) -> bool:
    """æ£€æµ‹æ˜¯å¦ä¸ºGeminiæ¨¡å‹"""
    if not model_name:
        return False
    model_lower = model_name.lower()
    return "gemini" in model_lower

def is_gemini_openai_compatible_request(chat_input) -> bool:
    """æ£€æµ‹æ˜¯å¦ä¸ºä½¿ç”¨OpenAIå…¼å®¹æ ¼å¼çš„Geminiæ¨¡å‹è¯·æ±‚"""
    return is_gemini_model(chat_input.model)

def supports_multimodal_content(model_name: str) -> bool:
    """æ£€æµ‹æ¨¡å‹æ˜¯å¦æ”¯æŒå¤šæ¨¡æ€å†…å®¹ï¼ˆéŸ³é¢‘ã€è§†é¢‘ã€å›¾åƒï¼‰
    
    æ³¨æ„ï¼šæ ¹æ®ç”¨æˆ·è¦æ±‚ï¼Œä¸å†å¯¹æ¨¡å‹ç±»å‹è¿›è¡Œåˆ¤æ–­ï¼Œ
    æ‰€æœ‰æ¨¡å‹éƒ½è¢«è§†ä¸ºæ”¯æŒå¤šæ¨¡æ€å†…å®¹ï¼Œè®©æ¨¡å‹è‡ªå·±å†³å®šå¦‚ä½•å¤„ç†ã€‚
    """
    # å§‹ç»ˆè¿”å› Trueï¼Œä¸å†é™åˆ¶æ¨¡å‹ç±»å‹
    return True

def is_zhipu_official_api(api_address_or_provider: str) -> bool:
    """
    æ£€æµ‹æ˜¯å¦ä¸ºæ™ºè°±AIå®˜æ–¹åŸŸæˆ–å…¶å­åŸŸï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦åº”äº¤ç”±å®˜æ–¹ web_search å·¥å…·å¤„ç†ï¼‰
    å…¸å‹åŸŸåï¼šopen.bigmodel.cn
    """
    if not api_address_or_provider:
        return False
    try:
        addr = api_address_or_provider.strip().lower()
        return "bigmodel.cn" in addr
    except Exception:
        return False

def is_google_official_api(api_address: str) -> bool:
    """æ£€æµ‹æ˜¯å¦ä¸ºGoogleå®˜æ–¹APIåœ°å€"""
    if not api_address:
        return False
    api_lower = api_address.lower()
    return "generativelanguage.googleapis.com" in api_lower or "aiplatform.googleapis.com" in api_lower

MAX_IMAGE_DIMENSION = 2048

def resize_and_encode_image_sync(image_bytes: bytes) -> str:
    """
    Resizes an image if it exceeds max dimensions and encodes it to Base64.
    This is a CPU-bound function and should be run in a thread.
    """
    if not Image:
        # Pillow not installed, just encode without resizing using Python's base64
        import base64
        return base64.b64encode(image_bytes).decode('utf-8').replace('\n', '')

    try:
        with Image.open(BytesIO(image_bytes)) as img:
            if img.width > MAX_IMAGE_DIMENSION or img.height > MAX_IMAGE_DIMENSION:
                img.thumbnail((MAX_IMAGE_DIMENSION, MAX_IMAGE_DIMENSION))
                
                output_buffer = BytesIO()
                # Preserve original format if possible, default to JPEG for wide compatibility
                img_format = img.format if img.format in ['JPEG', 'PNG', 'WEBP', 'GIF'] else 'JPEG'
                img.save(output_buffer, format=img_format)
                image_bytes = output_buffer.getvalue()

        import base64
        return base64.b64encode(image_bytes).decode('utf-8').replace('\n', '')
    except Exception as e:
        logger.error(f"Failed to resize or encode image: {e}", exc_info=True)
        # Fallback to encoding the original bytes if processing fails
        import base64
        return base64.b64encode(image_bytes).decode('utf-8').replace('\n', '')


async def handle_openai_compatible_request(
    chat_input: ChatRequestModel,
    uploaded_documents: List[UploadFile],
    fastapi_request_obj: Request,
    http_client: httpx.AsyncClient,
    request_id: str,
):
    log_prefix = f"RID-{request_id}"
    
    # ===== é»˜è®¤æ–‡æœ¬é…ç½®è‡ªåŠ¨æ³¨å…¥ï¼ˆå‰ç«¯ä¸å¯è§ï¼‰=====
    # è§¦å‘æ¡ä»¶ï¼šprovider æ˜ç¡®ä¸º"é»˜è®¤"æˆ–"default"ç­‰ï¼Œæˆ–è€… API key ä¸ºç©º
    def _is_default_text_provider(p: str) -> bool:
        if not isinstance(p, str):
            return False
        pl = p.strip().lower()
        return pl in ("é»˜è®¤", "default", "default_text")
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦ä½¿ç”¨é»˜è®¤é…ç½®ï¼šproviderä¸º"é»˜è®¤" æˆ– API keyä¸ºç©º
    should_use_default = (
        _is_default_text_provider(chat_input.provider or "") or
        not chat_input.api_key or
        (isinstance(chat_input.api_key, str) and not chat_input.api_key.strip())
    )
    
    if should_use_default:
        # åœ°å€ä¸å¯†é’¥ä»æœ¬åœ°ç¯å¢ƒæ³¨å…¥ï¼›ä¸¥ç¦å°†å¯†é’¥å†™å…¥ä»“åº“
        if DEFAULT_TEXT_API_URL:
            chat_input.api_address = DEFAULT_TEXT_API_URL
        if DEFAULT_TEXT_API_KEY:
            chat_input.api_key = DEFAULT_TEXT_API_KEY
        logger.info(f"{log_prefix}: [CHAT DEFAULT] Using default text API. url={chat_input.api_address}, key_provided={bool(DEFAULT_TEXT_API_KEY)}")
    
    # Read all file contents into memory immediately, as the file stream can be closed.
    multimodal_parts_in_memory = []
    document_texts = []
    # æ£€æµ‹æ˜¯å¦ä¸ºGeminiæ¨¡å‹ä»¥å¯ç”¨å¢å¼ºçš„å¤šæ¨¡æ€æ”¯æŒ
    is_gemini_request = is_gemini_openai_compatible_request(chat_input)
    # æ£€æµ‹æ˜¯å¦ä¸ºGoogleå®˜æ–¹API
    is_official_google_api = is_google_official_api(chat_input.api_address or "")
    
    if uploaded_documents:
        for doc_file in uploaded_documents:
            content_type = doc_file.content_type.lower() if doc_file.content_type else ""
            
            # ğŸ”¥ ä¿®å¤ï¼šOpenAIå…¼å®¹APIä¸æ”¯æŒå¤šç§æ–‡æ¡£çš„å¤šæ¨¡æ€å¤„ç†ï¼Œéœ€è¦èµ°æ–‡æœ¬æå–è·¯å¾„
            document_types = [
                "application/pdf",
                "text/plain", 
                "application/msword",  # .doc
                "application/vnd.openxmlformats-officedocument.wordprocessingml.document",  # .docx
                "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",  # .xlsx
                "application/vnd.ms-excel",  # .xls
                "application/vnd.openxmlformats-officedocument.presentationml.presentation",  # .pptx
                "application/vnd.ms-powerpoint",  # .ppt
                "text/html",
                "text/xml",
                "application/xml", 
                "application/json",
                "text/csv",
                "text/markdown",
                "text/rtf",
                "application/rtf"
            ]
            is_document_type = content_type in document_types
            should_use_multimodal = content_type in GEMINI_SUPPORTED_UPLOAD_MIMETYPES and not is_document_type
            
            # å¤„ç†éŸ³é¢‘/è§†é¢‘/å›¾åƒç­‰çœŸæ­£çš„å¤šæ¨¡æ€æ–‡ä»¶
            if should_use_multimodal:
                # å¦‚æœæ˜¯Geminiæ¨¡å‹ä½†ä½¿ç”¨ç¬¬ä¸‰æ–¹APIï¼Œç»™å‡ºè­¦å‘Š
                if is_gemini_request and not is_official_google_api and content_type in (AUDIO_MIME_TYPES + VIDEO_MIME_TYPES):
                    logger.warning(f"{log_prefix}: Using third-party API '{chat_input.api_address}' for Gemini model with audio/video content. This may not work properly.")
                try:
                    await doc_file.seek(0)
                    file_bytes = await doc_file.read()
                    file_size = len(file_bytes)
                    
                    logger.info(f"{log_prefix}: Processing multimodal file '{doc_file.filename}' ({file_size / 1024 / 1024:.2f} MB) for model {chat_input.model}")
                    
                    multimodal_parts_in_memory.append({
                        "content_type": content_type,
                        "data": file_bytes,
                        "type": "inline_data",
                        "filename": doc_file.filename,
                        "file_size": file_size
                    })
                    logger.info(f"{log_prefix}: Staged multimodal file '{doc_file.filename}' ({content_type}, {file_size / 1024 / 1024:.2f} MB) for processing with {chat_input.model}.")
                except Exception as e:
                    logger.error(f"{log_prefix}: Failed to read multimodal file {doc_file.filename} into memory: {e}", exc_info=True)
            # Process other documents for text extraction
            else:
                temp_file_path = ""
                try:
                    temp_file_path = os.path.join(TEMP_UPLOAD_DIR, f"{request_id}-{uuid.uuid4()}-{doc_file.filename}")
                    await doc_file.seek(0)
                    with open(temp_file_path, "wb") as f:
                        f.write(await doc_file.read())
                    
                    extracted_text = await extract_text_from_uploaded_document(
                        uploaded_file_path=temp_file_path,
                        mime_type=doc_file.content_type,
                        original_filename=doc_file.filename
                    )
                    if extracted_text:
                        document_texts.append(extracted_text)
                        logger.info(f"{log_prefix}: Successfully extracted text from document '{doc_file.filename}'.")
                except Exception as e:
                    logger.error(f"{log_prefix}: Failed to process document for text extraction {doc_file.filename}: {e}", exc_info=True)
                finally:
                    if temp_file_path and os.path.exists(temp_file_path):
                        os.remove(temp_file_path)

    async def event_stream_generator() -> AsyncGenerator[bytes, None]:
        final_messages_for_llm: List[Dict[str, Any]] = []
        user_query_for_search = ""
        processing_state: Dict[str, Any] = {}
        upstream_ok = False
        first_chunk_received = False

        # SSE prelude to nudge proxies to start sending immediately
        # Send a minimal comment event so clients can render promptly
        try:
            yield b":ok\n\n"  # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨çœŸæ­£çš„æ¢è¡Œç¬¦ï¼Œä¸æ˜¯å­—é¢å­—ç¬¦ä¸²
        except Exception:
            # If prelude fails we still continue streaming
            pass

        try:
            # --- Final Refactored Processing Logic ---

            # 1. Prepare context from newly uploaded files
            full_document_context = ""
            if document_texts:
                full_document_context = "\n\n".join(document_texts)
                full_document_context = f"--- Document Content ---\n{full_document_context}\n--- End Document ---\n\n"
            
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ç¬¬ä¸‰æ–¹APIå¤„ç†GeminiéŸ³é¢‘/è§†é¢‘ï¼Œç»™å‡ºç‰¹æ®Šæç¤º
            if (is_gemini_request and not is_official_google_api and
                multimodal_parts_in_memory and
                any(part.get("content_type", "") in AUDIO_MIME_TYPES + VIDEO_MIME_TYPES for part in multimodal_parts_in_memory)):
                
                api_domain = chat_input.api_address or "third-party service"
                warning_message = f"""âš ï¸ éŸ³é¢‘/è§†é¢‘å¤„ç†å¯èƒ½ä¸ç¨³å®š

æ‚¨æ­£åœ¨ä½¿ç”¨ç¬¬ä¸‰æ–¹APIæœåŠ¡ ({api_domain}) å¤„ç†Geminiæ¨¡å‹çš„éŸ³é¢‘/è§†é¢‘å†…å®¹ã€‚

**å¯èƒ½çš„é—®é¢˜ï¼š**
â€¢ ç¬¬ä¸‰æ–¹æœåŠ¡å¯èƒ½ä¸å®Œå…¨æ”¯æŒGeminiçš„å¤šæ¨¡æ€åŠŸèƒ½
â€¢ éŸ³é¢‘/è§†é¢‘å¤„ç†å¯èƒ½è¢«æ¨¡å‹æ‹’ç»æˆ–å¤„ç†ä¸å½“

**å»ºè®®è§£å†³æ–¹æ¡ˆï¼š**
1. ä½¿ç”¨Googleå®˜æ–¹APIåœ°å€ï¼š`https://generativelanguage.googleapis.com/v1beta/openai/`
2. æˆ–è€…åˆ‡æ¢åˆ°æ”¯æŒéŸ³é¢‘/è§†é¢‘çš„å…¶ä»–æ¨¡å‹ï¼ˆå¦‚GPT-4oï¼‰

æˆ‘ä»¬ä»ä¼šå°è¯•å¤„ç†æ‚¨çš„è¯·æ±‚ï¼Œä½†ç»“æœå¯èƒ½ä¸ç†æƒ³ã€‚"""
                
                yield orjson_dumps_bytes_wrapper(AppStreamEventPy(type="error", message=warning_message).model_dump(by_alias=True, exclude_none=True))

            new_multimodal_parts_for_openai: List[Dict[str, Any]] = []
            if multimodal_parts_in_memory:
                encoding_tasks = []
                valid_parts = []
                
                for part in multimodal_parts_in_memory:
                    content_type = part["content_type"]
                    file_size = part.get("file_size", 0)
                    filename = part.get("filename", "unknown")
                    
                    # å¯¹äºå¤§è§†é¢‘æ–‡ä»¶ï¼ˆ>20MBï¼‰ï¼Œè·³è¿‡Base64ç¼–ç ï¼Œå»ºè®®ä½¿ç”¨GeminiåŸç”ŸAPI
                    if content_type in VIDEO_MIME_TYPES and file_size > 20 * 1024 * 1024:
                        logger.warning(f"{log_prefix}: Large video file '{filename}' ({file_size / 1024 / 1024:.2f} MB) is too large for OpenAI compatible format.")
                        
                        # å‘é€é”™è¯¯ä¿¡æ¯åˆ°å‰ç«¯
                        error_message = f"Large video file '{filename}' ({file_size / 1024 / 1024:.1f} MB) is too large for OpenAI compatible format. Please use Gemini native API for large videos."
                        yield orjson_dumps_bytes_wrapper(AppStreamEventPy(type="error", message=error_message).model_dump(by_alias=True, exclude_none=True))
                        
                        document_texts.append(f"""[Large Video File: {filename} ({file_size / 1024 / 1024:.1f} MB)]

This video file is too large for OpenAI compatible format processing.

For better video processing, please use:
- Gemini native API endpoint (generativelanguage.googleapis.com)
- This will enable File API upload for large videos

The video was uploaded but cannot be analyzed in OpenAI compatible mode due to size limitations.""")
                        continue
                    
                    # å¯¹äºä¸­ç­‰å¤§å°çš„è§†é¢‘æ–‡ä»¶ï¼ˆ>5MBï¼‰ï¼Œç»™å‡ºè­¦å‘Š
                    elif content_type in VIDEO_MIME_TYPES and file_size > 5 * 1024 * 1024:
                        logger.warning(f"{log_prefix}: Medium video file '{filename}' ({file_size / 1024 / 1024:.2f} MB) may take longer to process.")
                        yield orjson_dumps_bytes_wrapper(AppStreamEventPy(type="status_update", stage=f"Processing video file ({file_size / 1024 / 1024:.1f} MB)...").model_dump(by_alias=True, exclude_none=True))
                    
                    valid_parts.append(part)
                    if content_type in SUPPORTED_IMAGE_MIME_TYPES_FOR_OPENAI:
                        encoding_tasks.append(asyncio.to_thread(resize_and_encode_image_sync, part["data"]))
                    else:
                        # å¯¹äºéŸ³é¢‘/è§†é¢‘æ–‡ä»¶ï¼Œç›´æ¥è¿›è¡ŒBase64ç¼–ç ï¼ˆç¡®ä¿æ— æ¢è¡Œç¬¦ï¼‰
                        encoding_tasks.append(asyncio.to_thread(lambda data: base64.b64encode(data).decode('utf-8').replace('\n', ''), part["data"]))

                if encoding_tasks:
                    try:
                        logger.info(f"{log_prefix}: Starting encoding for {len(encoding_tasks)} files...")
                        
                        # å‘é€å¤„ç†çŠ¶æ€æ›´æ–°
                        yield orjson_dumps_bytes_wrapper(AppStreamEventPy(type="status_update", stage="Processing video/audio files...").model_dump(by_alias=True, exclude_none=True))
                        
                        # æ·»åŠ è¶…æ—¶ä¿æŠ¤ï¼Œé˜²æ­¢å¤§è§†é¢‘æ–‡ä»¶ç¼–ç å¡æ­»
                        encoded_results = await asyncio.wait_for(
                            asyncio.gather(*encoding_tasks),
                            timeout=120.0  # 2åˆ†é’Ÿè¶…æ—¶
                        )
                        logger.info(f"{log_prefix}: Encoding completed successfully for {len(encoded_results)} files")
                        
                        # å‘é€ç¼–ç å®ŒæˆçŠ¶æ€
                        yield orjson_dumps_bytes_wrapper(AppStreamEventPy(type="status_update", stage="Files processed, analyzing...").model_dump(by_alias=True, exclude_none=True))
                        
                        for i, encoded_data_bytes in enumerate(encoded_results):
                            encoded_data_str = encoded_data_bytes if isinstance(encoded_data_bytes, str) else encoded_data_bytes.decode('utf-8')
                            content_type = valid_parts[i]["content_type"]
                            filename = valid_parts[i].get("filename", "unknown")
                            
                            # æ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼Œä¸ºä¸åŒç±»å‹çš„åª’ä½“ä½¿ç”¨æ­£ç¡®çš„æ ¼å¼
                            if content_type in AUDIO_MIME_TYPES:
                                # éŸ³é¢‘ä½¿ç”¨input_audioæ ¼å¼ï¼Œæ ¹æ®å®˜æ–¹æ–‡æ¡£è§„èŒƒ
                                audio_format = get_audio_format_from_mime_type(content_type)
                                new_multimodal_parts_for_openai.append({
                                    "type": "input_audio",
                                    "input_audio": {
                                        "data": encoded_data_str,
                                        "format": audio_format
                                    }
                                })
                                logger.info(f"{log_prefix}: Successfully encoded and added audio {filename} ({content_type}) using input_audio format with format '{audio_format}'.")
                            elif content_type in VIDEO_MIME_TYPES:
                                # è§†é¢‘ä»ä½¿ç”¨image_urlæ ¼å¼ï¼ˆæ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼Œè§†é¢‘ä¹Ÿé€šè¿‡data URIå¤„ç†ï¼‰
                                data_uri = f"data:{content_type};base64,{encoded_data_str}"
                                new_multimodal_parts_for_openai.append({"type": "image_url", "image_url": {"url": data_uri}})
                                logger.info(f"{log_prefix}: Successfully encoded and added video {filename} ({content_type}) using image_url format with data URI.")
                            else:
                                # å›¾åƒå’Œå…¶ä»–ç±»å‹ä½¿ç”¨image_urlæ ¼å¼
                                data_uri = f"data:{content_type};base64,{encoded_data_str}"
                                new_multimodal_parts_for_openai.append({"type": "image_url", "image_url": {"url": data_uri}})
                                logger.info(f"{log_prefix}: Successfully encoded and added {filename} ({content_type}) using image_url format. Data URI length: {len(data_uri)}")
                            
                    except asyncio.TimeoutError:
                        logger.error(f"{log_prefix}: File encoding timed out after 2 minutes. This usually happens with large video files.")
                        
                        # å‘é€è¶…æ—¶é”™è¯¯ä¿¡æ¯åˆ°å‰ç«¯
                        timeout_message = "Video/Audio file encoding timed out after 2 minutes. Please try with smaller files or use Gemini native API for large files."
                        yield orjson_dumps_bytes_wrapper(AppStreamEventPy(type="error", message=timeout_message).model_dump(by_alias=True, exclude_none=True))
                        
                        document_texts.append("[Video/Audio files were uploaded but encoding timed out. Please try with smaller files or use Gemini native API for large files.]")
                    except Exception as e:
                        logger.error(f"{log_prefix}: Error during file encoding: {e}", exc_info=True)
                        
                        # å‘é€ç¼–ç é”™è¯¯ä¿¡æ¯åˆ°å‰ç«¯
                        encoding_error_message = f"Error processing uploaded files: {str(e)}"
                        yield orjson_dumps_bytes_wrapper(AppStreamEventPy(type="error", message=encoding_error_message).model_dump(by_alias=True, exclude_none=True))
                        
                        document_texts.append(f"[Multimodal files were uploaded but could not be processed due to encoding error: {str(e)}]")

            # 2. Build the final message list, preserving history correctly
            # --- Refactored Message Processing Logic ---
            for i, msg_abstract in enumerate(chat_input.messages):
                msg_dict: Dict[str, Any] = {"role": msg_abstract.role}
                is_last_user_message = (i == len(chat_input.messages) - 1 and msg_abstract.role == "user")

                content_parts = []
                
                # Step 1: Convert message content into a unified 'parts' format
                if isinstance(msg_abstract, SimpleTextApiMessagePy):
                    if msg_abstract.content:
                        content_parts.append({"type": "text", "text": msg_abstract.content})
                elif isinstance(msg_abstract, PartsApiMessagePy):
                    for part in msg_abstract.parts:
                        if isinstance(part, PyTextContentPart) and part.text:
                            content_parts.append({"type": "text", "text": part.text})
                        elif isinstance(part, PyInlineDataContentPart):
                            # æ£€æŸ¥æ˜¯å¦ä¸ºéŸ³é¢‘ç±»å‹ï¼Œä½¿ç”¨æ­£ç¡®çš„æ ¼å¼
                            if part.mime_type in AUDIO_MIME_TYPES:
                                audio_format = get_audio_format_from_mime_type(part.mime_type)
                                content_parts.append({
                                    "type": "input_audio",
                                    "input_audio": {
                                        "data": part.base64_data,
                                        "format": audio_format
                                    }
                                })
                            else:
                                # è§†é¢‘å’Œå›¾åƒä½¿ç”¨image_urlæ ¼å¼
                                data_uri = f"data:{part.mime_type};base64,{part.base64_data}"
                                content_parts.append({"type": "image_url", "image_url": {"url": data_uri}})
                        elif isinstance(part, PyInputAudioContentPart):
                            # æ ¹æ®å®˜æ–¹æ–‡æ¡£æ ¼å¼å¤„ç†éŸ³é¢‘å†…å®¹
                            content_parts.append({
                                "type": "input_audio",
                                "input_audio": {
                                    "data": part.data,
                                    "format": part.format
                                }
                            })

                # Step 2: If it's the last user message, inject new context
                if is_last_user_message:
                    # Extract user query for web search BEFORE adding context
                    user_query_for_search = " ".join([p.get("text", "") for p in content_parts if p.get("type") == "text"]).strip()

                    # Prepend document context to the text parts
                    if full_document_context:
                        # Find first text part to prepend to, or insert at the beginning
                        text_part_index = next((idx for idx, p in enumerate(content_parts) if p["type"] == "text"), -1)
                        if text_part_index != -1:
                            content_parts[text_part_index]["text"] = full_document_context + content_parts[text_part_index]["text"]
                        else:
                            content_parts.insert(0, {"type": "text", "text": full_document_context})
                    
                    # Append new multimodal parts (e.g., uploaded images)
                    if new_multimodal_parts_for_openai:
                        logger.info(f"{log_prefix}: Adding {len(new_multimodal_parts_for_openai)} multimodal parts to the last user message")
                        content_parts.extend(new_multimodal_parts_for_openai)
                        for idx, part in enumerate(new_multimodal_parts_for_openai):
                            if part["type"] == "input_audio":
                                logger.info(f"{log_prefix}: Multimodal part {idx+1}: type={part['type']}, format={part['input_audio']['format']}")
                            else:
                                logger.info(f"{log_prefix}: Multimodal part {idx+1}: type={part['type']}, data_uri_length={len(part.get('image_url', {}).get('url', ''))}")

                # Step 3: Finalize the content for the message payload
                if not content_parts:
                    msg_dict["content"] = ""
                elif len(content_parts) == 1 and content_parts[0]["type"] == "text":
                    msg_dict["content"] = content_parts[0]["text"]
                else:
                    msg_dict["content"] = content_parts
                
                final_messages_for_llm.append(msg_dict)

            # --- Web Search Logic ---
            # ğŸ”¥ NEW: Skip server-side search injection for Gemini models - let native tool handle it
            if chat_input.use_web_search and user_query_for_search:
                if is_gemini_request:
                    # For Gemini models, we rely on the native google_search tool injected in openai_builder
                    logger.info(f"{log_prefix}: Gemini model detected with web search enabled. Using native google_search tool (injected in request builder).")
                    yield orjson_dumps_bytes_wrapper(AppStreamEventPy(
                        type="status_update", 
                        stage="Using Gemini native search tool..."
                    ).model_dump(by_alias=True, exclude_none=True))
                    # Do NOT inject search results into messages - let Gemini handle it natively
                # è‹¥ä¸ºæ™ºè°±å®˜æ–¹APIï¼ˆæˆ–provideræŒ‡å‘bigmodel.cnï¼‰ï¼Œä½¿ç”¨æ™ºè°±Web Search API
                elif is_zhipu_official_api(chat_input.api_address or "") or is_zhipu_official_api(chat_input.provider or ""):
                    yield orjson_dumps_bytes_wrapper(AppStreamEventPy(type="status_update", stage="Using Zhipu Web Search API...").model_dump(by_alias=True, exclude_none=True))
                    try:
                        # ä½¿ç”¨æ™ºè°±Web Search API
                        zhipu_search = ZhipuWebSearchService(http_client)
                        
                        # ä»APIå¯†é’¥ä¸­æå–ï¼ˆå‡è®¾åœ¨headeræˆ–payloadä¸­ï¼‰
                        api_key = chat_input.api_key
                        if not api_key:
                            logger.warning(f"{log_prefix}: No API key found for Zhipu Web Search")
                            yield orjson_dumps_bytes_wrapper(AppStreamEventPy(type="status_update", stage="Web search skipped (no API key)...").model_dump(by_alias=True, exclude_none=True))
                        else:
                            # è°ƒç”¨æ™ºè°±æœç´¢
                            formatted_text, web_search_results = await zhipu_search.search_and_format(
                                api_key=api_key,
                                search_query=user_query_for_search,
                                request_id=request_id,
                                search_engine="search_pro",  # ä½¿ç”¨é«˜çº§ç‰ˆæœç´¢å¼•æ“
                                count=5,
                                search_recency_filter="year",
                                content_size="high"
                            )
                            
                            if web_search_results:
                                # å‘é€æœç´¢ç»“æœåˆ°å‰ç«¯
                                yield orjson_dumps_bytes_wrapper(AppStreamEventPy(
                                    type="web_search_results", 
                                    results=[{
                                        "index": i + 1,
                                        "title": r.get("title", ""),
                                        "href": r.get("url", ""),
                                        "snippet": r.get("snippet", "")
                                    } for i, r in enumerate(web_search_results)]
                                ).model_dump(by_alias=True, exclude_none=True))
                                
                                # å°†æœç´¢ç»“æœæ³¨å…¥åˆ°æ¶ˆæ¯ä¸­
                                if final_messages_for_llm and final_messages_for_llm[-1].get("role") == "user":
                                    last_user_msg = final_messages_for_llm[-1]
                                    content = last_user_msg.get("content", "")
                                    search_context = f"\n\nã€è”ç½‘æœç´¢ç»“æœã€‘\n{formatted_text}\n\nè¯·åŸºäºä»¥ä¸Šæœç´¢ç»“æœå›ç­”ï¼š\n"
                                    
                                    if isinstance(content, str):
                                        last_user_msg["content"] = search_context + content
                                    elif isinstance(content, list):
                                        # å¦‚æœæ˜¯å¤šæ¨¡æ€æ¶ˆæ¯ï¼Œåœ¨æ–‡æœ¬éƒ¨åˆ†å‰æ·»åŠ æœç´¢ä¸Šä¸‹æ–‡
                                        for part in content:
                                            if part.get("type") == "text":
                                                part["text"] = search_context + part["text"]
                                                break
                                
                                logger.info(f"{log_prefix}: Zhipu Web Search completed with {len(web_search_results)} results")
                                yield orjson_dumps_bytes_wrapper(AppStreamEventPy(type="status_update", stage="Answering with search results...").model_dump(by_alias=True, exclude_none=True))
                            else:
                                logger.info(f"{log_prefix}: Zhipu Web Search returned no results")
                                yield orjson_dumps_bytes_wrapper(AppStreamEventPy(type="status_update", stage="No search results, answering directly...").model_dump(by_alias=True, exclude_none=True))
                    
                    except Exception as e:
                        logger.error(f"{log_prefix}: Zhipu Web Search failed: {e}", exc_info=True)
                        yield orjson_dumps_bytes_wrapper(AppStreamEventPy(type="status_update", stage="Search failed, answering directly...").model_dump(by_alias=True, exclude_none=True))
                else:
                    yield orjson_dumps_bytes_wrapper(AppStreamEventPy(type="status_update", stage="Searching web...").model_dump(by_alias=True, exclude_none=True))
                    try:
                        # å¯¹äºéGeminiæ¨¡å‹ï¼Œä½¿ç”¨è‡ªå®šä¹‰æœç´¢æ¥æä¾›æœç´¢ç»“æœ
                        search_results = await perform_web_search(user_query_for_search, request_id)
                        if search_results:
                            yield orjson_dumps_bytes_wrapper(AppStreamEventPy(type="web_search_results", results=search_results).model_dump(by_alias=True, exclude_none=True))
                            
                            # ç®€åŒ–æœç´¢ä¸Šä¸‹æ–‡ï¼Œè®©AIæ›´åŸç”Ÿåœ°å¤„ç†æœç´¢ç»“æœ
                            search_context_content = f"""Search results for "{user_query_for_search}":

"""
                            for i, res in enumerate(search_results):
                                search_context_content += f"""{i + 1}. {res.get('title', 'N/A')}
{res.get('snippet', 'N/A')}
{res.get('href', 'N/A')}

"""
                            
                            # ä¸å†æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯ï¼Œç›´æ¥å°†æœç´¢ç»“æœä½œä¸ºç”¨æˆ·æ¶ˆæ¯çš„ä¸€éƒ¨åˆ†
                            if final_messages_for_llm and final_messages_for_llm[-1].get("role") == "user":
                                last_user_msg = final_messages_for_llm[-1]
                                content = last_user_msg.get("content", "")
                                if isinstance(content, str):
                                    last_user_msg["content"] = search_context_content + "\n\n" + content
                                elif isinstance(content, list):
                                    # å¦‚æœæ˜¯å¤šæ¨¡æ€æ¶ˆæ¯ï¼Œåœ¨æ–‡æœ¬éƒ¨åˆ†å‰æ·»åŠ æœç´¢ä¸Šä¸‹æ–‡
                                    for i, part in enumerate(content):
                                        if part.get("type") == "text":
                                            part["text"] = search_context_content + "\n\n" + part["text"]
                                            break
                            
                            logger.info(f"{log_prefix}: Injected web search context for non-Gemini OpenAI compatible request.")
                            yield orjson_dumps_bytes_wrapper(AppStreamEventPy(type="status_update", stage="Answering...").model_dump(by_alias=True, exclude_none=True))
                    except Exception as e_search:
                        logger.error(f"{log_prefix}: Web search failed, proceeding without search context. Error: {e_search}", exc_info=True)
                        yield orjson_dumps_bytes_wrapper(AppStreamEventPy(type="status_update", stage="Web search failed, answering directly...").model_dump(by_alias=True, exclude_none=True))

            # --- API Request and Streaming ---
            
            def build_openai_compatible_url(address: str) -> str:
                """æ ¹æ®è§„åˆ™æ„å»ºæœ€ç»ˆçš„ä¸Šæ¸¸ URLï¼Œå…¼å®¹ OpenAI ä¸æ™ºè°± BigModel."""
                if not address:
                    # å¦‚æœåœ°å€ä¸ºç©ºï¼Œåˆ™ä½¿ç”¨é»˜è®¤çš„OpenAIåœ°å€å’Œè·¯å¾„
                    from ..core.config import DEFAULT_OPENAI_API_BASE_URL, OPENAI_COMPATIBLE_PATH
                    return f"{DEFAULT_OPENAI_API_BASE_URL.rstrip('/')}/{OPENAI_COMPATIBLE_PATH.lstrip('/')}"
                
                address = address.strip()
                # å»é™¤ç”¨äºç¦ç”¨é»˜è®¤æ‹¼æ¥çš„ '#'
                if address.endswith("#"):
                    return address.rstrip("#")
                
                from urllib.parse import urlparse
                parsed = urlparse(address if "://" in address else f"http://{address}")
                host_lower = (parsed.netloc or "").lower()
                path = parsed.path or "/"
                
                # æ™ºè°±å®˜æ–¹åŸŸåï¼šå¼ºåˆ¶ä½¿ç”¨ /api/paas/v4/chat/completions
                if "bigmodel.cn" in host_lower:
                    base = address.rstrip("/")
                    # è‹¥ç”¨æˆ·å·²æä¾›å®Œæ•´ chat/completions è·¯å¾„ï¼Œåˆ™å°Šé‡ç”¨æˆ·è¾“å…¥
                    if "/api/paas/v4/chat/completions" in path or base.endswith("/api/paas/v4/chat/completions"):
                        return address
                    # è‹¥ç”¨æˆ·æä¾›åˆ°åŸŸæˆ–ä»»æ„éè¯¥è·¯å¾„ï¼Œç»Ÿä¸€è§„èŒƒåˆ°å®˜æ–¹è·¯å¾„
                    return f"{base}/api/paas/v4/chat/completions"
                
                # å…¶ä»–åŸŸåæŒ‰ OpenAI å…¼å®¹è§„èŒƒ
                if address.endswith("/"):
                    return f"{address}chat/completions"
                
                # å¦‚æœåœ°å€ä¸­ä¸åŒ…å«è·¯å¾„ï¼Œåˆ™æ·»åŠ é»˜è®¤è·¯å¾„
                if not parsed.path or parsed.path == "/":
                    return f"{address.rstrip('/')}/v1/chat/completions"
                
                # å…¶ä»–æƒ…å†µï¼Œç›´æ¥ä½¿ç”¨ç”¨æˆ·æä¾›çš„åœ°å€
                return address

            # å¿½ç•¥ request_builder è¿”å›çš„URLï¼Œåªä½¿ç”¨å®ƒçš„ headers å’Œ payload
            _, current_api_headers, current_api_payload = prepare_openai_request(
                request_data=chat_input,
                processed_messages=final_messages_for_llm,
                request_id=request_id,
                system_prompt=chat_input.system_prompt
            )

            final_api_url = build_openai_compatible_url(chat_input.api_address)
            logger.info(f"{log_prefix}: Final target URL built for OpenAI compatible request: {final_api_url} (Original: {chat_input.api_address})")
            
            # ğŸ”¥ è°ƒè¯•ï¼šè®°å½•è¯·æ±‚payloadçš„å…³é”®ä¿¡æ¯
            try:
                logger.info(f"{log_prefix}: Request payload keys: {list(current_api_payload.keys())}")
                if "messages" in current_api_payload:
                    messages = current_api_payload["messages"]
                    logger.info(f"{log_prefix}: Messages count: {len(messages)}")
                    for i, msg in enumerate(messages):
                        if "content" in msg and isinstance(msg["content"], list):
                            content_types = [part.get("type", "unknown") for part in msg["content"]]
                            logger.info(f"{log_prefix}: Message {i} content types: {content_types}")
                        elif "content" in msg:
                            content_preview = str(msg["content"])[:100]
                            logger.info(f"{log_prefix}: Message {i} content preview: {content_preview}")
            except Exception as debug_error:
                logger.error(f"{log_prefix}: Could not debug payload: {debug_error}")

            # 401é”™è¯¯é‡è¯•é€»è¾‘
            max_retries = 5
            retry_count = 0
            last_error_message = ""
            
            while retry_count < max_retries:
                try:
                    async with http_client.stream("POST", final_api_url, headers=current_api_headers, json=current_api_payload, timeout=API_TIMEOUT) as response:
                        upstream_ok = response.status_code == 200
                        
                        # 401é”™è¯¯ï¼šé‡è¯•
                        if response.status_code == 401:
                            retry_count += 1
                            error_body = await response.aread()
                            error_text = error_body.decode('utf-8', errors='ignore')
                            logger.warning(f"{log_prefix}: OpenAI compatible 401 error (attempt {retry_count}/{max_retries}): {error_text[:200]}")
                            
                            if retry_count < max_retries:
                                wait_time = min(2 ** (retry_count - 1), 10)
                                logger.info(f"{log_prefix}: Retrying in {wait_time} seconds...")
                                await asyncio.sleep(wait_time)
                                continue
                            else:
                                last_error_message = f"APIå¯†é’¥éªŒè¯å¤±è´¥ (401): å·²é‡è¯•{max_retries}æ¬¡ä»ç„¶å¤±è´¥ã€‚è¯·æ£€æŸ¥æ‚¨çš„APIå¯†é’¥æ˜¯å¦æ­£ç¡®ã€‚è¯¦ç»†é”™è¯¯: {error_text[:200]}"
                                logger.error(f"{log_prefix}: {last_error_message}")
                                # æŠ›å‡ºå¼‚å¸¸ä»¥ä¾¿å¤–å±‚æ•è·å¹¶å‘é€é”™è¯¯
                                response.raise_for_status()

                        # å…¶ä»–HTTPé”™è¯¯
                        if not upstream_ok:
                            error_body = await response.aread()
                            logger.error(f"{log_prefix}: HTTP {response.status_code} error body: {error_body.decode('utf-8', errors='ignore')[:1000]}")
                            response.raise_for_status()
                        
                        # ä¸Šæ¸¸æˆåŠŸ â†’ è¯»å–æµ
                        buffer = bytearray()
                        done = False
                        async for chunk in response.aiter_bytes():
                            if done:
                                break
                            buffer.extend(chunk)
                            while not done:
                                separator_pos = buffer.find(b'\n\n')
                                if separator_pos == -1:
                                    break

                                message_data = buffer[:separator_pos]
                                buffer = buffer[separator_pos + 2:]

                                if not message_data.strip():
                                    continue

                                for line in message_data.split(b'\n'):
                                    line = line.strip()
                                    if line.startswith(b"data:"):
                                        json_str = line[5:].strip()
                                        if json_str == b"[DONE]":
                                            done = True
                                            break
                                        try:
                                            decoded = json_str.decode('utf-8', errors='ignore')
                                            sse_data = orjson.loads(decoded)
                                            
                                            # Gemini å›¾åƒç”Ÿæˆç‰¹æ®Šå¤„ç†
                                            if "gemini-2.5-flash-image-preview" in chat_input.model.lower():
                                                try:
                                                    content = sse_data.get("choices", [{}])[0].get("delta", {}).get("content", "")
                                                    if content:
                                                        image_data = orjson.loads(content)
                                                        image_url = image_data.get("url") or f"data:image/png;base64,{image_data.get('b64_json')}"
                                                        if "url" in image_data or "b64_json" in image_data:
                                                            yield orjson_dumps_bytes_wrapper(AppStreamEventPy(type="image_generation", image_url=image_url).model_dump(by_alias=True, exclude_none=True))
                                                            sse_data["choices"][0]["delta"]["content"] = ""
                                                except (orjson.JSONDecodeError, IndexError):
                                                    pass

                                            async for event in process_openai_like_sse_stream(sse_data, processing_state, request_id, chat_input):
                                                yield _sse_event_bytes(AppStreamEventPy(**event))
                                        except (orjson.JSONDecodeError, UnicodeDecodeError) as e:
                                            logger.warning(f"{log_prefix}: Skipping corrupted SSE line: {e}. Line: {line[:100]}...")
                                        except Exception as e:
                                            logger.error(f"{log_prefix}: Unexpected error during SSE line handling: {e}", exc_info=True)
                        # æˆåŠŸå®Œæˆä¸€æ¬¡æµï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                        break
                except (httpx.RequestError, httpx.HTTPStatusError) as e_outer:
                    # æ•è·æ‰€æœ‰è¯·æ±‚ç›¸å…³çš„é”™è¯¯
                    is_upstream_ok = False
                    is_first_chunk_received = False
                    
                    # å¦‚æœæ˜¯401é”™è¯¯ä¸”å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
                    if isinstance(e_outer, httpx.HTTPStatusError) and e_outer.response.status_code == 401 and retry_count >= max_retries:
                        error_message = last_error_message
                    else:
                        error_message = str(e_outer)

                    logger.error(f"{log_prefix}: Failed to establish upstream stream or HTTP error: {error_message}", exc_info=True)
                    async for error_event in handle_stream_error(e_outer, request_id, is_upstream_ok, is_first_chunk_received, custom_message=error_message):
                        yield error_event
                    return
        finally:
            is_upstream_ok_final = 'upstream_ok' in locals() and upstream_ok
            use_custom_sep = False
            async for final_event in handle_stream_cleanup(processing_state, request_id, is_upstream_ok_final, use_custom_sep, chat_input.provider):
                yield final_event
            
            # No temp files to delete as we are reading into memory
            pass

    return StreamingResponse(
        event_stream_generator(),
        media_type="text/event-stream",
        headers={
            # Prevent intermediary/proxy buffering or transformations
            "Cache-Control": "no-cache, no-transform",
            "Connection": "keep-alive",
            # Disable Nginx/X-Accel buffering if present
            "X-Accel-Buffering": "no",
        },
    )